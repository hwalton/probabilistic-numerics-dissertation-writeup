\documentclass[12pt]{article}
\usepackage{fontspec}

\usepackage{afterpage} % For deferring content to a specific later page
\usepackage{xcolor}
\usepackage{mathrsfs}

\setmainfont{Times New Roman} % Times New Roman, Arial, Calibri

\usepackage{setspace}
\setstretch{1.15}

\usepackage{pdflscape}

\usepackage{longtable}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}

% Define a command to set the font size and make it bold
\newcommand{\captionfonts}{\fontsize{10pt}{12pt}\selectfont\bfseries}

% Apply the custom font command to the captions
\captionsetup[figure]{
    font=small,
    format=plain,
    labelfont=bf,
    textfont=bf,
    labelsep=period
}

\captionsetup[table]{
    font=small,
    format=plain,
    labelfont=bf,
    textfont=bf,
    labelsep=period
}

\DeclareCaptionFont{bf}{\captionfonts}
\captionsetup[figure]{font=bf}
\usepackage{float}
\usepackage{placeins}
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{references.bib}

\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

\usepackage{pdfpages}

\usepackage{listings}
\usepackage{xcolor}
% Define colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup the listings package
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\usepackage{array} % For better column formatting

\lstset{style=mystyle, language=Python}
\captionsetup[lstlisting]{font=bf}

\usepackage{amsmath} % For mathematical features

\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\usepackage{gensymb}
\usepackage{nomencl}
\usepackage{chngcntr}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{hyperref}
\counterwithin{figure}{section}
\counterwithin{equation}{section}
\counterwithin{table}{section}

\title{Identifying Dynamic Systems with Probabilistic Numerics}
\author{Harvey Walton}
\date{\today}

%Repeated Text
\newcommand{\ndiFigCaption}[1]{The rectangle method for finding the #1 bound of the integral of the standard Gaussian (normal) distribution between -2 and 1.}

\hyphenpenalty=700
\exhyphenpenalty=700

\makenomenclature

% Define nomenclature groupings
\renewcommand{\nomgroup}[1]{%
    \ifthenelse{\equal{#1}{G}}{\item[\textbf{Greek Symbols}]}{%
        \ifthenelse{\equal{#1}{L}}{\item[\textbf{Latin Symbols}]}{}}}

\setlength{\nomlabelwidth}{2cm}
%\renewcommand{\nomlabel}[1]{\textbf{#1}\hfil} % Bold the symbols and right-align
%\setlength{\nomitemsep}{-\parsep} % Reduce the space between items

\begin{document}
    \pagenumbering{roman}

    \thispagestyle{empty}
    \includepdf[pages=1, frame, scale=1.09, pagecommand={}, offset=0 -35]{figures/titlepage.pdf}

    \section{Summary}
    \textcolor{red}{rework the conclusion}

    \printnomenclature

    \newpage
    \tableofcontents
    \newpage

    \pagenumbering{arabic}

    \section{Background and Understanding of the Problem}

    \subsection{Dynamic Systems}
    In mechanical engineering, the field of dynamics studies how systems, like structures and fluids, respond and evolve over time under varying forces.
    These forces can range from aerodynamic pressures to impacts from waves or road irregularities.
    Excessive vibration or oscillation of machinery is known to reduce service life and in extreme cases cause catastrophic failure.
    Therefore, by studying these dynamic systems they could be designed to perform better, be safer and to have lower maintenance costs.
    However, the field of dynamics is far from being mastered since there are many types of problems that cannot yet be solved.

    To begin with, here are some examples of dynamic systems:
    \begin{enumerate}[listparindent=\parindent]
        \item Firstly, a motorcycle suspension system is a combination of springs, dampers and linkages that connect the motorcycle chassis to its wheels.
        Its primary purpose is to keep the tyres in contact with the ground, and without proper suspension, the tyres would lose traction when encountering bumps especially when braking, accelerating or cornering~\cite{UTI2020MotorcycleSuspension}.
        It is designed to allow some relative motion between the chassis and the wheels in order to absorb impacts while at the same time trying to maintain its equilibrium position, and zero velocity (both relative to the chassis).

        The dampers in the suspension produce a force proportional to the velocity that acts to slow the wheel down.
        Viscous dampers do this by pushing a liquid though a small orifice as the wheel moves.
        Damping can be optimised (or made ``critical'') by bringing the system back to equilibrium after a disturbance as fast as possible.
        Too little damping, and the wheel will overshoot and oscillate back and forth before reaching equilibrium.
        Too much damping, and although the wheel will not oscillate, it will reach equilibrium more slowly than optimal.

        Similarly, the springs create stiffness in the system that produces a force with a magnitude proportional to the displacement of wheel which acts to return the wheel to its equilibrium position.
        Too high of a stiffness means that the system will not have enough time to absorb the impact softly, but too low means that the bike's responses to steering inputs are slow and unreactive.

        Studying this dynamic system could allow for the prediction of behaviour based on varying these parameters.
        This would enable optimisation without trial and error, conserving resources during the research and development phase of the product lifecycle.

        \item Secondly, offshore wind turbines are known to experience high cycle fatigue (HCF) due to a relentless random forcing the waves that repeatedly crash against the base of its tower.
        This is a phenomenon that causes any micro-defects to grow into cracks over time due to cyclic loading below the elastic limit of the material, which reduces the life of the structure due to increased risk of mechanical failure.

        The rate at which this occurs increases with the amplitude of the stresses in the structure, which in turn increase with the amplitude of oscillation in response to this cyclic loading.
        If a periodic force is applied to any system with a specific frequency known as a resonant frequency, the amplitude of the oscillation may become very large even for a small amplitude force, leading to an excessive rate of high cycle fatigue.
        The same can happen in the case of the wind turbine that receives a random force with an \textit{average} frequency similar to this resonant frequency.
        It is even possible for the stresses in a structure to become so large that they exceed the ultimate tensile strength of the material, causing a sudden mechanical failure and the collapse of the turbine.

        Therefore, to make structures safer and reduce maintenance costs, it would be valuable to be able to model their dynamic properties (such as this resonant frequency) from system parameters (such as the stiffness of the tower) so that the frequencies at which they occur can be designed away from the expected frequencies of the periodic forces.

        \item Finally, an aircraft wing bends in response to aerodynamic forces.
        However, the forces themselves change as the wing twists and bends.
        The interdependence of these two effects creates complicated interactions that are difficult to model accurately.
        But it is important to do so because if at high airspeed, energy from the wing forcing is added to the system faster than it can be dissipated, the positive feedback loop can lead to a phenomenon called aerodynamic flutter where the oscillations become larger and larger until they cause catastrophic failure.
    \end{enumerate}

    \subsection{Identifying Dynamic Systems}

    So, dynamic systems have parameters such as mass, stiffness and damping coefficients which are fundamental characteristics of the system.
    ``Forwards modelling'' is when the observable behaviour of the system, known as the system properties, are predicted from the system parameters using physical laws.
    These properties include the resonant frequencies which are the forcing frequencies that produce a peak in amplitude of excitation, and the damping ratios which compare the damping coefficient to that of critical damping at each resonant frequency.

    Often a distinction is made between linear and nonlinear systems.
    Only linear systems comply with the principle of superposition, which states that ``when two or more waves simultaneously pass through a point, the disturbance at the point is given by the sum of the disturbances each wave would produce in absence of the other waves''~\cite{StudyComSuperposition}.
    This means that if you double the input force over time, the response over time is exactly double, but otherwise identical.
    This is not the case for nonlinear systems (such as the aircraft wing in the example above) where non-compliance with the principle of superposition causes the system properties to distort as the magnitude of the input force is increased.
    This, in turn, causes the response to become highly complex and chaotic, meaning that it is highly sensitive to initial conditions.
    This is why forward modelling is currently too difficult for systems that are non-linear.

    Unfortunately, everything is non-linear in real life.
    Only in rare cases are systems close enough to being linear that they can be approximated as such.
    What's worse is that even if a model did exist to solve the nonlinear system, it would be of no use because typically not even the system parameters are known to be able to input them into the model.
    This is why the field of dynamics has not been mastered, and all that can currently be done for medium to highly non-linear systems - \textit{which is almost all systems} - is to take real life data measurements from an existing system and identify the system properties and parameters that best fit the observed data through what is known as ``inverse modelling''.
    However, even this process is fraught with issues and uncertainty.

    \subsection{Classically Quantifying Uncertainty with Error Bounds}
    Identifying system properties and parameters using this inverse modelling approach results in values that are not exact, but have an uncertainty associated with them that comes from multiple sources.
    The first source of uncertainty is noise that is always present to some degree in sampled input data due to the measuring devices - typically accelerometers - that are used to capture the force input and corresponding response from the system.
    Furthermore, even if the true input and response could be sampled exactly, the non-identifiability problem states that since there are multiple sets of system parameters that could give the same set of observed outputs, it's impossible to uniquely determine them.
    Similarly, many algorithms make use of mathematical objects (such as matrix pseudoinverses) which have multiple alternatives - any of which could be used - where the correct choice is subjective.
    This noise and lack of uniqueness is reflected in the outputted system properties and parameters as uncertainty.


    Time passes continuously forever, and the governing equations of dynamic systems are defined mathematically in the continuous domain.
    In theory, these could be solved ``analytically'' to give an exact mathematical solution in the continuous domain.
    But in practice, data sampled from the real world is usually discrete, meaning that it is defined by a finite set of points separated by finite time intervals.
    In addition, many sets of governing of equations are currently too complex to solve analytically for systems that are not highly idealised.
    This is why when modelling, problems are classically solved using ``numerical methods'', which are discrete approximations of analytical solutions of the governing equations.
    Here, uncertainty of values is often quantified using error bounds derived from sensitivity analysis.
    For instance, the error bound of a stiffness could be represented as $0.1 \pm 0.02 \, Nm^{-1}$, indicating the expected range of the true value.
    This process involves estimating the uncertainty of input data, usually from the measuring equipment's resolution or variability in sample data.
    From these estimates, upper and lower bounds of the output parameters can be calculated.
    However, this is not straightforward and typically requires additional techniques and considerations beyond the core algorithm.


    \subsection{The Fourier Transform and the Frequency Response Function (FRF)}

    When inverse modelling during the analysis of dynamic systems, any collected time series data is almost always converted into the frequency domain, meaning the output becomes a function of frequency instead of a function of time.
    This is because it gives the physical meaning of the system properties a clear visual interpretation.
    Specifically, the Frequency Response Function (FRF) is computed, which is the ratio between the response and input in the frequency domain.
    This function is complex, which means it has two components that provide information regarding the amplitude (magnitude) and phase (lag) of the response versus the input.
    If the system parameters (mass, damping coefficient or stiffness) change, so do the system properties (resonant frequency and damping ratio).
    This is visible in the FRF in Figure~\ref{fig:frequency-response-function}, where a larger resonant frequency creates a peak in excitation at a higher frequency, and a lower damping ratio results in a sharper peak.
    Note the phase shifts by $\pi$ rad ($180\degree$) across a resonant frequency.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\linewidth]{figures/frequency-response-function/frequency-response-function_}
        \caption{The magnitude and phase of $H(\xi)$, a frequency response function of a single-degree-of-freedom (SDOF) system for an acceleration measured in response to an input force, showing how changing its system parameters mass $m$ [$kg$], damping coefficient $c$ [$N \cdot s \cdot m^{-1}$] and stiffness $k$ [$N \cdot m^{-1}$] affects the system parameters resonant frequency $\xi_r$ [$rad \cdot s^{-1}$] and damping ratio $\zeta$ [$-$].}
        \label{fig:frequency-response-function}
    \end{figure}

    The decomposition of the input and response functions from the time domain to the frequency domain uses a linear operator called the Fourier Transform, $F(\xi)$, as defined in Equation~\ref{eq:ft}

    \begin{equation}
        F(\xi) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} f(t) e^{-i \xi t} \, dt\label{eq:ft}
    \end{equation}
    \nomenclature[L05]{$F(\xi)$}{Fourier Transform of $f(t)$}
    \nomenclature[G08]{$\xi$}{Angular frequency}


    \noindent where $\xi$ is the angular frequency and $f(t)$ is the time domain latent function being operated on.
    This is an extremely powerful analytical technique that is at the core of signal processing, which in turn is a set of techniques that are widely used in engineering to analyse, manipulate and interpret experimental data that changes over time in order to extract useful information or improve them for a specific purpose.
    However, for it to be applied in its base form to a function, this function must be defined for every point in time in the continuous domain $(-\infty, \infty)$.
    Therefore, when working with sampled real world data, an approximation of this function is classically found using a numerical method called a Discrete Fourier Transform (DFT), specifically the Fast Fourier Transform (FFT).

    \subsection{The Fast Fourier Transform Algorithm}
    Although discovering this fast way to compute the Discrete Fourier Transform revolutionised many engineering disciplines such as structural health monitoring, image compression, signal processing, and control theory~\cite{Byjus2023}, its implementation has a number of challenges that have to be carefully handled.
    Firstly, the Fast Fourier Transform (FFT) assumes the input signal is finite in length and periodic, which can lead to a phenomenon called spectral leakage if the signal is not an exact multiple of the chosen total time period~\cite{MathStackExchange2023}.\nomenclature[L04]{FFT}{Fast Fourier Transform}
    This can cause sharp resonance peaks to be smoothed out in the output because energy that should be concentrated at a peak frequency is spread across a range of frequencies nearby, and is typically mitigated by using a window such as a Hanning window to decrease the magnitude of the time domain signal at either end of the data to smoothly bring the data to zero at the boundary between the end and start, thus minimizing and abrupt changes that could lead to distortion of the spectrum.
    Secondly, the FFT can be highly sensitive to noise in the data, meaning a small amount of noise can create large fluctuations in the shape of the frequency spectrum, masking the true underlying frequencies and modal properties~\cite{MathStackExchange2023}.
    Similarly, since the time domain and resulting frequency domain data is discrete, the basic algorithm can exhibit large variability in the amplitude and frequency of peaks depending on if the sampled frequencies fall exactly on the peak or either side of it, which in turn can affect modal properties and parameters calculated downstream of this data in the context of dynamics.
    Finally, there is a highest ``Nyquist'' frequency that is half the sampling frequency, for which any higher frequencies present in the time domain signal will cause aliasing.
    This is where frequencies that are too high are reflected back at this frequency.

    However, over time this spectra will need to be measured more accurately and at higher frequencies in the engineering industry.
    For example, as scramjet engines are developed in the aerospace industry for faster hypersonic aircraft, the vibration endured will be stronger and at higher frequencies than in engines available today.
    This can lead to failure of components from fatigue, so being able to accurately measure the FFT spectra at these higher frequencies allows engineers to design effective damping systems and structures that can withstand the stresses over the lifespan of the vehicle while minimising weight of material.

    Currently, spectra are found with the FFT more accurately and at higher frequencies by simply collecting more and more data, but this is not always feasible since there are limits to the available resources such as time, energy and capital required to collect this data.
    In the example of the scramjet engine, hypersonic wind tunnel tests only last on the order of milliseconds~\cite{Mec463} to reduce the cost of energy used and to prevent damage to the engine from the resulting high temperatures.
    This provides very little time for vibration data to be collected.
    Similarly, if measurement instruments with higher sampling frequencies are required to bring the limiting Nyquist frequency above the high frequency vibrations, the cost of this equipment, and the computation required to process the data will increase.


    \subsection{Probabilistic Numerics and the Gaussian Process}
    This paper explores the potential advantages of a new paradigm of ``probabilistic numerics'' over traditional numerical methods that may allow one to identify dynamic systems more accurately from less data.
    Here, probabilistic reasoning is incorporated into the process of computation by treating ``a numerical problem as one of statistical inference instead''~\cite{ProbabilisticNumerics2023}.
    The uncertainty of outputs is modelled with a probability distribution as opposed to the error bounds as in traditional numerical methods, which may improve robustness to noise, among other things.
    This paper does this through the implementation of a promising type of machine learning model called a Gaussian Process (GP) to find the Fourier Transform of a dynamic system.

    A GP uses noisy, discrete sampled data to model the probability distribution of a continuous ``latent'' function, $f(x)$, that represents the underlying relationship between an input, $x$, and an output, $f(x)$.\nomenclature[L06]{$f(x)$}{Latent function of the variable $x$}\nomenclature[L08]{GP}{Gaussian Process}
    In essence, it is ``a collection of random variables, any finite number of which have a joint Gaussian distribution''~\cite{rasmussen2006gaussian} which describes how the probability distribution of each variable depends on the other variables.
    These variables are outputs of the latent function at a specific input point and include both the training data (noisy observed samples at points in time), and the prediction points at which the true value is unknown.

    Assuming a mean of the latent function of zero, and a noisy observed force, $y_j = f(x_j)+\epsilon_j$, where $\epsilon_j \sim \mathcal{N}(0, \sigma^2_y)$, this joint distribution of the observed data and the latent, noise-free function at the test points is given~\cite{murphy2023probabilistic} by Equation~\ref{eq:joint-distribution}

    \begin{equation}
        \left[
            \begin{array}{c}
                \mathbf{y} \\
                \mathbf{f}_*
            \end{array}
            \right]
        \sim \mathcal{N} \left(
        \left\{
        \begin{array}{c}
            \boldsymbol{\mu}_X \\
            \boldsymbol{\mu}_*
        \end{array}
        \right\},
        \left[
            \begin{array}{cc}
                \mathbf{K}_{X,X} + \sigma^2_y \mathbf{I} & \mathbf{K}_{X,*} \\
                \mathbf{K}_{X,*}^T & \mathbf{K}_{*,*}
            \end{array}
            \right]
        \right)\label{eq:joint-distribution}
    \end{equation}

    \nomenclature[L22]{$y_j$}{Noisy observed output based on the latent function, $f(x)$}
    \nomenclature[L21]{$\mathbf{y}$}{A vector of $y_j$ values from a set of training points}
    \nomenclature[L07]{$\mathbf{f}_*$}{is a vector of function values at a set of test points $\mathbf{X}_*$}
    \nomenclature[L09]{$\mathbf{I}$}{Identity matrix}
    \nomenclature[L11]{$\mathbf{K}_{X,X}$}{Covariance matrix for training input data, $X$}
    \nomenclature[L12]{$\mathbf{K}_{X,*}$}{Covariance matrix between training inputs and test inputs, $\mathbf{X}$ and $\mathbf{X_*}$ respectively}
    \nomenclature[L13]{$\mathbf{K}_{*,*}$}{Covariance matrix for test inputs, $\mathbf{X}_*$}
    \nomenclature[G01]{$\epsilon_j$}{Noise in observation, normally distributed with a mean of zero and a variance of $\sigma^2_y$}
    \nomenclature[L23]{$\boldsymbol{X}$}{A vector of training point inputs}
    \nomenclature[L24]{$\boldsymbol{X}_*$}{A vector of test point inputs}
    \nomenclature[G07]{$\sigma^2_y$}{Variance of observation noise}
    \nomenclature[G03]{$\boldsymbol{\mu}_X, \boldsymbol{\mu}_*$}{Vectors containing the mean of the distribution of values of the latent function, at the training points, $\mathbf{X}$, and test points $\mathbf{X}_*$ respectively}
    \nomenclature[L10]{$\mathbf{K}_{\mathbf{A},\mathbf{B}}$}{Covariance matrix of the kernel function, $k(a,b)$, evaluated for each pair of elements a, b in vectors $\mathbf{A}$, $\mathbf{B}$}

    \noindent where $\epsilon_j$ is the observational noise; $\mathbf{y}$ is a vector of $y_j$ values from a set of training points; $\boldsymbol{\mu}_X$ and $\boldsymbol{\mu}_*$ are a vectors containing the mean of the distribution of values of the latent function, at the training and test points, $\mathbf{X}$ and  $\mathbf{X}_*$ respectively; $\mathbf{f}_*$ is a vector of function values at a set of test points $\mathbf{X}_*$; $\mathbf{I}$ is the identity matrix and $\mathbf{K}_{X,X}$, $\mathbf{K}_{X,*}$ \& $\mathbf{K}_{*,*}$ are a type of covariance matrix, $\mathbf{K}(\mathbf{A},\mathbf{B})$, based on the ``kernel'' function, $k(a,b)$ that is evaluated for each pair elements in the vectors $(\mathbf{X},\mathbf{X})$, $(\mathbf{X},\mathbf{X}_*)$ and $(\mathbf{X}_*,\mathbf{X}_*)$ respectively.
    Note that block matrix notation is used where matrices are conjoined to form a larger block matrix.

    The kernel is type of function that measures the similarity between each pair of inputs based on a set of hyperparameters and is chosen to reflect any prior knowledge about the problem domain.

    These hyperparameters need to be optimised by minimizing the Negative Log Marginal Likelihood (NLML), as defined~\cite{murphy2023probabilistic} in Equation~\ref{eq:NLML}\nomenclature[L19]{NLML}{Negative Log Marginal Likelihood}

    \begin{equation}
        -\log p(\mathbf{D}|\boldsymbol{\theta}) = \frac{1}{2} (\mathbf{y} - \boldsymbol{\mu}_X)^T \mathbf{K}_{\sigma}^{-1} (\mathbf{y} - \boldsymbol{\mu}_X) + \frac{1}{2} \log |\mathbf{K}_{\sigma}| + \frac{N}{2} \log(2\pi)\label{eq:NLML}
    \end{equation}

    \nomenclature[L03]{$\mathbf{D}$}{Training data}
    \nomenclature[G02]{$\boldsymbol{\theta}$}{Set of hyperparameters}
    \nomenclature[L14]{$\mathbf{K}_{\sigma}$}{Covariance matrix with noise term, $\mathbf{K}_{X,X} + \sigma^2_y \mathbf{I}$}
    \nomenclature[L18]{$N$}{Number of training or testing points}

    \noindent where $\mathbf{D}$ is the training data; $N$ is the number of training points; $\boldsymbol{\theta}$ is the set of hyperparameters and $\mathbf{K}_{\sigma} = \mathbf{K}_{X,X} + \sigma^2_y \mathbf{I}$ is the covariance matrix with the noise term.
    This measures how well the model explains the observed data for a given set of hyperparameters while penalizing complexity to avoid overfitting, which is when the model fits the training data so well that it generalises poorly to new test data.
    This can happen when the model has not only learnt the underlying function but also its associated noise, which will not be consistent between datasets due to its random nature.
    Optimisation in this manor works by identifying the hyperparameter values that are most likely to produce the observed training data.

    To predict the latent function at one or multiple points, the ``posterior'' probability distribution of the function is determined by updating the initial prior assumptions using the training data.
    This process is called conditioning the joint Gaussian prior distribution on the observations~\cite{rasmussen2006gaussian}, as shown in Equation~\ref{eq:18.51}, where $\boldsymbol{\mu}_{*\vert \mathbf{X}}$ and $\boldsymbol{\Sigma}_{*\vert \mathbf{X}}$ are the conditional mean and covariance of the GP, given the training data, $\mathbf{X}$ as shown in Equation~\ref{eq:18.52} and Equation~\ref{eq:18.53} respectively.

    \begin{equation}
        p(\mathbf{f}^* \vert \mathbf{D}, \mathbf{X}^*) = \mathcal{N}(\mathbf{f}^* \vert \boldsymbol{\mu}_{*\vert \mathbf{X}}, \boldsymbol{\Sigma}_{*\vert \mathbf{X}})\label{eq:18.51}
    \end{equation}

    \begin{equation}
        \boldsymbol{\mu}_{*\vert \mathbf{X}} = \boldsymbol{\mu}_* + \mathbf{K}_{X,*}^T (\mathbf{K}_{X,X} + \sigma^2_y \mathbf{I})^{-1} (\mathbf{y} - \boldsymbol{\mu}_X)\label{eq:18.52}
    \end{equation}

    \begin{equation}
        \boldsymbol{\Sigma}_{*\vert \mathbf{X}} = \mathbf{K}_{*,*} - \mathbf{K}_{X,*}^T (\mathbf{K}_{X,X} + \sigma^2_y \mathbf{I})^{-1} \mathbf{K}_{X,*}\label{eq:18.53}
    \end{equation}

    \nomenclature[G04]{$\boldsymbol{\mu}_{*\vert \mathbf{X}}$}{Conditional mean of the Gaussian Process, given the training data, $\mathbf{X}$}
    \nomenclature[G05]{$\boldsymbol{\Sigma}_{*\vert \mathbf{X}}$}{Conditional covariance of the Gaussian Process, given the training data, $\mathbf{X}$}

    Note that this GP is closed under the action of linear operators which means that it can be described using only a combination of operations that comply with the principle of superposition.
    It is also defined in the continuous domain $(-\infty, \infty)$, so the Fourier Transform can be taken analytically without needing to resort to the discrete FFT\@.
    Incredibly, since the Fourier Transform is a linear operator~\cite{Jidling2017}, the result of this operation on the GP is yet another GP. The joint distribution of the two can therefore be written as shown in Equation~\ref{eq:linear-trans}

    \begin{equation}
        \left[
            \begin{array}{c}
                \mathbf{y}(t) \\
                \mathbf{g}(\xi)
            \end{array}
            \right]
        \sim \mathcal{N} \left(
        \left\{
        \begin{array}{c}
            \boldsymbol{\mu}_X(t) \\
            \mathcal{F}[{\boldsymbol{\mu}_*}](\xi)
        \end{array}
        \right\},
        \left[
            \begin{array}{cc}
                \mathbf{K}_{X,X} & \mathcal{F}_{*} [\boldsymbol{K}_{X,*}] \\
                \mathcal{F}_{*} [\boldsymbol{K}_{X,*}^T] & \mathcal{F}_{*1} [\mathcal{F}_{*2} [\boldsymbol{K}_{*1,*2}]]
            \end{array}
            \right]
        \right)\label{eq:linear-trans}
    \end{equation}

    \noindent where $\mathcal{F}_*$, $\mathcal{F}_{*1}$ and $\mathcal{F}_{*2}$ are (elementwise) Fourier Transforms with respect to test points in vectors $\mathbf{X_{*}}$, $\mathbf{X_{*1}}$ and $\mathbf{X_{*2}}$ respectively, where $\mathbf{X_{*1}}$ and $\mathbf{X_{*2}}$ are separate instantiations of the vector $\mathbf{X_{*}}$. \textcolor{red}{CHANGE THIS TO $\mathbf{F}$?}

    Like before, this distribution can be conditioned to find the conditional mean and covariance of the Fourier Transform at a specific set of desired frequencies, given the training data, $\mathbf{X}$.


    Modelling signals in this way provides an inherent mechanism to account for uncertainty and allows this information to be preserved naturally in the covariance matrix when performing linear operations on the data.
    Furthermore, the noise term  $\epsilon_j \sim \mathcal{N}(0, \sigma^2_y)$ explicitly models the observational noise, giving the GP a way to distinguish between the signal and noise components.
    This means that the use of probabilistic numerics may allow better identification of dynamic systems since it may allow for Fourier Transform  (which such identification uses extensively) to be performed with better robustness to noise.

    \section{Aims and Objectives}
    \subsection{Aim}
    The main aim of this paper is to improve identification of properties of a dynamic system from noisy of time series data by improving noise resilience when converting the data to the frequency domain.

    \subsection{Objectives}
    This overarching aim can be broken down into three objectives:
        \begin{enumerate}
            \item Create a working Gaussian Process model, which involves:
                \begin{itemize}
                    \item Sourcing some time series signal data.
                    \item Choosing and creating a kernel.
                    \item Implementing optimisation of kernel hyperparameters. \label{item:nll}
                    \item Creating a prediction function to predict the outputs of new inputs. \label{item:predict}
                \end{itemize}
            \item Adjust this model to allow predictions of the mean and covariance functions in the frequency domain by finding their Fourier Transforms in closed form. \label{item:FT}
            \item Test the robustness to noise of this model compared to the conventional Fast Fourier Transform in the context of a dynamic system. \label{noise-resilience}
            \item Explore any other quantitative and qualitative advantages and disadvantages of the new model.
        \end{enumerate}

    \section{Creating the Gaussian Process Model}
    The first objective was completed by implementing the Gaussian Process (GP) model in the programming language Python.
    \subsection{The Data}
    Initially, training data was required to test the fit of the GP model during its creation.
    For this, the acceleration of a single degree of freedom (SDOF) system in free vibration in the time domain was used.
    If this data was collected experimentally, some noise would always be present, so instead it was simulated instead with system parameters $M = 1 \, kg$; $C = 0.3 \, Nsm^{-1}$; $K = 100 \, Nm^{-1}$.
    This allowed the levels of noise to be reduced to zero, as shown in the plot of ``Training Data'' in Figure~\ref{fig:basic-data-time-domain}, to allow for a broader range of test cases.


%    \begin{equation}
%        \ddot{x}(t) = -X e^{-\zeta \xi_n t} \xi_n^2 \cos(\xi_d t + \phi) \label{eq:sdof-free-vib}
%    \end{equation}
%
%    \noindent Where $\ddot{x}(t) is the acceleration$

    \textcolor{red}{add equation of motion??}

%    \begin{figure}[ht]
%        \centering
%        \includegraphics[width=1.0\linewidth]{figures/input-response-plot/input-response-plot.png}
%        \caption{The time series training data, and the mean and standard deviation of the GPs prediction of its latent function. Note that the prediction functions are continuous but must be evaluated at a discrete set of points in order to be plotted.}
%        \label{fig:input-response-plot}
%    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=1.0\linewidth]{figures/basic-data-time-domain/basic-data-time-domain.png}
        \caption{The basic training dataset of time domain acceleration of a simulated SDOF system consisting of $128$ datapoints at a sample rate of $32 \, Hz$ using system parameters $M = 1 \, kg$; $C = 0.3 \, Nsm^{-1}$; $K = 100 \, Nm^{-1}$ with no noise added, and the time domain prediction functions of the GP model fit to this data.}
        \label{fig:basic-data-time-domain}
    \end{figure}


    \subsection{The Kernel}
    A Squared Exponential Kernel~\cite{duvenaud2014kernel} was chosen to reflect the nature of the sample data, both of which are continuous and smooth, with no sudden jumps.
    In addition, the kernel has only two hyperparameters.
    This low number speeds up their optimisation process compared to alternative kernels.
    This function is defined in Equation~\ref{eq:se-kernel}

    \begin{equation}
        k(a,b) = \sigma^2 \exp\left(-\frac{(a - b)^2}{2l^2}\right) \label{eq:se-kernel}
    \end{equation}
    \nomenclature[L15]{$k(a,b)$}{Kernel function, evaluating the similarity of points $a$ and $b$}
    \nomenclature[L02]{$a, b$}{Elements of vectors $\mathbf{A}, \mathbf{B}$ respectively}
    \nomenclature[L01]{$\mathbf{A}, \mathbf{B}$}{Vectors with elements $a, b$ - subcript indicates index of element}
    \nomenclature[G06]{$\sigma^2$}{The variance hyperparameter of the SE kernel}
    \nomenclature[L16]{$l$}{Length scale hyperparameter of the SE kernel}

    \noindent where $k(a,b)$ is the kernel function evaluated at elements $a_i$, $b_j$ of vectors $\mathbf{A}$, $\mathbf{B}$ respectively and $\sigma^2$ is the variance hyperparameter and $l$ is the length scale parameter.

    The variance hyperparameter, $\sigma^2$, determines the average distance of the function away from its mean.
    It is a simple scale factor present in all kernels.
    Finally, length hyperparameter, $l$, determines how smooth or wiggly the function is, i.e., how fast the function can change direction.

    \subsection{Hyperparameter Optimisation}
    An algorithm was designed to optimise the hyperparameters of the kernel by minimising the Negative Log Marginal Likelihood (NLML), for which a method was written (Appendix.~\ref{app:GPT}).
    It did this by modifying each hyperparameter randomly in turn and seeing if the NLML improved.
    This was repeated for up to $100$ iterations.

    Imagine if the hyperparameter was only updated the new NLML was lower than the previous.
    Since the shape of a cost function can be complex and highly dimensional, the algorithm could have got stuck in what is known as a local minimum and never find the global minimum.
    This would be analogous to a ball that ``tried'' to find the bottom of a valley by rolling directly downhill.
    If it got stuck in a small ditch half-way down the larger hill, it would never find the lowest point.

    Therefore, a mechanism was added to the algorithm to avoid this:
    If the new NLML was higher (i.e., worse) than the previous, the hyperparameter was maybe still updated, with a larger probability the closer the NLML was to the previous NLML\@.
    When the last iteration was complete, the lowest NLML of all the combinations hyperparameters was selected.
    This allowed the algorithm to explore after reaching a minimum to potentially find another part of the NLML function that was lower.

    \subsection{Predict Function}
    The predict function was constructed, which accurately calculated the mean and standard deviation of the latent function as shown by the ``Prediction Mean'' and ``Prediction Std Dev'' plots in Figure~\ref{fig:basic-data-time-domain} respectively.
    The mean of each prediction point is calculated using Equation~\ref{eq:18.52} directly, whereas the standard deviation of each point uses only the main diagonal of the covariance matrix resulting from Equation~\ref{eq:18.53}, which is square rooted in order to find the standard deviation.
    In combination, this predict function defines the probability distribution of the value of the latent function at any point, given the training data.
    Hence, when Gaussian noise ($\sim \mathcal{N}(0, 25^{2})$) was added to the training data, the standard deviation of the prediction became larger to take account for the uncertainty created by this noise, as shown in Figure~\ref{fig:noisy-data-time-domain}.

%    \begin{figure}[ht]
%%        \centering
%%        \includegraphics[width=1.0\linewidth]{figures/input-response-noise/input-response-noise.png}
%%        \caption{The mean and standard deviation of the prediction function of the GP when noise was added to the training data, showing that the standard deviation of the prediction increases when noise is added.}
%%        \label{fig:input-response-noise}
%%    \end{figure}


    \begin{figure}[h]
        \centering
        \includegraphics[width=1.0\linewidth]{figures/noisy-data-time-domain/noisy-data-time-domain.png}
        \caption{The mean and standard deviation of the prediction function of the GP when noise was added to the training data, showing that the standard deviation of the prediction increases.}
        \label{fig:noisy-data-time-domain}
    \end{figure}


    \section{Adjusting the Model for the Frequency Domain}

    \subsection{Finding the Fourier Transform of the Mean}
    Next, the function for the Fourier Transform of the prediction of the mean was constructed, which could be evaluated at any set of points in the domain $(-\infty, \infty)$.
    This was done using a neat method described by Ambrogioni et al~\cite{Ambrogioni2018} which is equivalent to conditioning the joint distribution in Equation~\ref{eq:linear-trans} but instead takes the Fourier Transform of the time domain predictive mean function directly.
    Remarkably, this involves taking the Fourier Transform of only the kernel (Equation~\ref{eq:se-kernel-ft}), provided that the Fourier shift theorem is first applied to ensure that the kernel is a function of a single variable.

    \begin{equation}
        \mathcal{K}(\xi,0) = \frac{\sigma^2 l}{\sqrt{2 \pi}} \exp\left(-\frac{l^2 \xi^2}{2}\right) \label{eq:se-kernel-ft}
    \end{equation}
    \nomenclature[L165]{$k(\xi,0)$}{Fourier Transform of the Squared Exponential Kernel function evaluated at $\xi$}

    \noindent where $\xi$ is the frequency and $\mathcal{K}(\xi,0)$ is the Fourier Transform of the Squared Exponential Kernel function evaluated at $\xi$.

    For the mean function written in the form shown in Equation~\ref{eq:mean-sum-time-domain}

    \begin{equation}
        \mathbf{\mu}(t) = \sum_{k} w_k \, k(t,t_k) \label{eq:mean-sum-time-domain}
    \end{equation}

    \noindent where $t_k$ is the $k^{th}$ element in the vector of training points $\mathbf{X}$; $t$ is an element in the vector of test points, $\mathbf{X}_*$ and $w_k$ is defined in Equation~\ref{eq:w_k}

    \begin{equation}
        w_k = \sum_{j} a_{kj} \, y_j \label{eq:w_k}
    \end{equation}

    \noindent where $a_{kj}$ is an element in matrix $\mathbf{A}$ defined in Equation~\ref{eq:A},

    \begin{equation}
         \mathbf{A} = (\mathbf{K}_{X,X} + \sigma^2_y \, \mathbf{I})^{-1} \label{eq:A}
    \end{equation}

    \noindent its Fourier Transform is shown to be Equation~\ref{eq:mean-sum-freq-domain}.

    \begin{equation}
        \mathcal{F}[{\mu}](\xi) = \mathcal{K}(\xi,0) \sum_k w_k \, e^{- i \xi t_k} \label{eq:mean-sum-freq-domain}
    \end{equation}

    \subsection{Finding the Fourier Transform of the Covariance}

    The same steps can be used to derive the equation for the Fourier Transform of the function for the main diagonal of the Covariance Matrix when written in the form shown in Equation~\ref{eq:var-sum-time-domain}

    \begin{equation}
        \Sigma(t, t) = k(t,t) - \sum_k v_k \, k(t,t_k) \label{eq:var-sum-time-domain}
    \end{equation}

    \noindent where $v_k$ is defined in Equation~\ref{eq:v_k}

    \begin{equation}
        v_k = \sum_j a_{kj} \, k(t,t_j) \label{eq:v_k}
    \end{equation}

    \noindent where $t_j$ is the $j^{th}$ element in the vector of training points $\mathbf{X}$.

    The resulting Fourier Transform can be written as shown in Equation~\ref{eq:var-sum-freq-domain}.

%    \begin{equation}
%        \mathcal{F}[\Sigma](\xi, \xi) = \frac{1}{2 \pi} e^{- i \xi^2} k(- \xi,0)  - \sum_k \sum_j a_{kj} \, e^{-i \xi (t_j - t_k)} \mathcal{K}(\xi,0)^2 \label{eq:var-sum-freq-domain}
%    \end{equation}

    \begin{equation}
        \mathcal{F}[\Sigma](\xi, \xi) = \mathcal{K}(\xi,0) \delta(\xi - \xi)  - \sum_k \sum_j a_{kj} \, e^{-i \xi (t_j - t_k)} \mathcal{K}(\xi,0)^2 \label{eq:var-sum-freq-domain}
    \end{equation}

%    Note that $k(- \xi, 0)$ is the original time domain kernel since the Fourier Transform applied to a function twice is equivalent to the original function reflected about zero and scaled by dividing by $2 \pi$.

    Only the Fourier Transform of the main diagonal of the covariance matrix was computed because hyperparameter training (which would require the full matrix) was completed in the time domain already, and only the diagonal is required to plot the standard deviation of the output predictions (which is the square root of the covariance).
    This decreased the compute requirements significantly, from $O(N^2)$ to $O(N)$, where N is the number of frequency predictions required.



    \section{Results and discussion}
    After this Gaussian Process Model was complete, it could then be setup and used in a variety of test cases to explore its advantages and disadvantages versus the Fast Fourier Transform.
    The dataset used to do this was from the model of the acceleration of the SDOF system in free vibration from Figure~\ref{fig:basic-data-time-domain}.
    The number of datapoints was extended to 1024 (at the same sample rate of $32 \, Hz$) to provide more information, allowing for more accurate results.

    \subsection{Basic Dataset}
    \subsubsection{Accuracy of the Fourier Transform} \label{Acc}
    Initially, no noise was added to validate the new approach of computing the Fourier Transform.
    Since the dataset was simulated using known parameters, the exact Fourier Transform of the latent function could be calculated analytically.
    This could be used to test the accuracy of the GP model, using the performance of the FFT (with a Hanning window) on the same system as a benchmark.
    Here, the Fourier Transform of the GP's prediction mean, as well as the FFT and the analytical Fourier Transform were calculated.
    Since the Fourier Transform is complex, to show it fully in a graph, it must be broken down into two components - either into its magnitude and phase, or into its real and imaginary parts.
    Both representations are shown for each of the three variants of the Fourier Transform in Figure~\ref{fig:basic-data-freq-domain}.

    \afterpage{
        \clearpage
        \begin{landscape}
            \begin{figure}[p]
                \centering
                \includegraphics[width=\linewidth]{figures/basic-data-freq-domain/basic-data-freq-domain.png}
                \caption{The analytical Fourier Transform, FFT and the Fourier Transform of the GP's prediction mean of the basic dataset of the SDOF system without noise, broken down into the magnitude and phase components, as well as the real and imaginary components.}
                \label{fig:basic-data-freq-domain}
            \end{figure}
        \end{landscape}
        \clearpage
    }

    First, the properties of the peak were compared to the exact simulated value for each.
    From observation of the magnitude plots, the GP Fourier Transform seems to more smoothly reach the peak, which resembles the shape of the analytical Fourier Transform more closely than the FFT.
    This suggests that it captures information relating to damping better, likely because the FFT's Hanning window would distort the signal's exponential decay in the time domain from the damping dissipating energy over time.
    Since the mean of the GP was sampled in the frequency domain in the same places as the FFT, the angular frequency of the peak - as measured by the sample with the largest magnitude - was identical, $10.01 \, rad \, s^{-1}$, within one sample interval of the analytical peak of $10.00 \, rad \, s^{-1}$.
    The percentage error in the magnitude of the peaks was $22.7 \, \%$ and $53.5 \, \%$ for the FFT and GP model respectively.

    Next, the Mean Squared Error (MSE) was used to compare the accuracy (lower is better) of each of the FFT and the Fourier Transform of the GP's prediction mean with respect to the analytically exact Fourier Transform.
    This algorithm finds the average of the squares of the differences between the points at each index position of two series, as shown in Equation~\ref{eq:mse}.

    \begin{equation}
        \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (a_i - b_i)^2 \label{eq:mse}
    \end{equation}

    \noindent where $a_i$, $b_i$ are the values of vectors $\mathbf{A}$, $\mathbf{B}$ at index i.

    The MSE for the magnitude component, as well as the real and imaginary parts of each of the two numerical methods is shown in Table~\ref{tab:basic-mse}, where the error of the magnitude GP model's Fourier transform was $1.22$ times larger than that of the FFT, but error real and imaginary parts were $4.42$ and $12.66$ times smaller respectively.

    \begin{table}[H]
        \centering % This centers the table on the page
        \setlength{\arrayrulewidth}{1pt} % Sets the thickness of the table borders
        \begin{tabular}{|c|c|c|} % 'c' for centered columns, '|' for vertical lines between columns
            \hline
            Basic Dataset & FFT & GP FT Mean \\ % Titles for the columns
            \hline
            Magnitude MSE [ms$^{-2}$] & $2.09$ & $2.55$ \\
            \hline
            Real Part MSE [ms$^{-2}$] & $7.59$ & $1.72$ \\
            \hline
            Imag Part MSE [ms$^{-2}$] & $10.57$ & $0.83$ \\
            \hline
        \end{tabular}
        \caption{A table showing the Mean Squared Error (MSE) for the components of both the FFT, and the Fourier Transform of the GP's prediction mean when compared to the analytically exact Fourier Transform for the basic dataset.}
        \label{tab:basic-mse}
    \end{table}

    Note that the MSE of the phase was not computed because numerical instabilities had a large effect in its values when the real and imaginary components were near zero, so it wouldn't have been meaningful.
    But overall, the performance of the GP model was comparable to the FFT, since neither was superior in every metric.
    Therefore, the GP model was shown to be a valid alternative to the FFT\@.

    \subsection{Robustness to Noise}
    The robustness to noise was tested (Objective~\ref{noise-resilience}) by calculating the MSE of the two methods when zero mean Gaussian noise with a standard deviation of $25 \, ms^{-2}$ was added to the signal.
    The components of the Fourier Transforms are shown in Figure~\ref{fig:noisy-data-freq-domain}.

    \afterpage{
        \clearpage
        \begin{landscape}
            \begin{figure}[p]
                \centering
                \includegraphics[width=\linewidth]{figures/noisy-data-freq-domain/noisy-data-freq-domain.png}
                \caption{The analytical Fourier Transform, FFT and the Fourier Transform of the GP's prediction mean when Gaussian noise with a standard deviation of $25 \, ms^{-2}$ was added to the signal, broken down into the magnitude and phase components, as well as the real and imaginary components.}
                \label{fig:noisy-data-freq-domain}
            \end{figure}
        \end{landscape}
        \clearpage
    }

    Table~\ref{tab:response-noise} shows that the GP model performed significantly better here, with the error of the magnitude, real part and imaginary part of the GP model's Fourier transform being $9.66$, $6.48$ and $11.64$ times smaller than the FFT respectively.

    \begin{table}[h]
        \centering % This centers the table on the page
        \setlength{\arrayrulewidth}{1pt} % Sets the thickness of the table borders
        \begin{tabular}{|c|c|c|} % 'c' for centered columns, '|' for vertical lines between columns
            \hline
            Noisy Dataset & FFT & GP FT Mean \\ % Titles for the columns
            \hline
            Magnitude MSE [ms$^{-2}$] & $22.80$ & $2.36$ \\
            \hline
            Real Part MSE [ms$^{-2}$] & $22.88$ & $3.53$ \\
            \hline
            Imag Part MSE [ms$^{-2}$] & $27.94$ & $2.40$ \\
            \hline
        \end{tabular}
        \caption{A table showing the Mean Squared Error (MSE) for the components of both the FFT, and the Fourier Transform of the GP's prediction mean when compared to the analytically exact Fourier Transform when Gaussian noise with a standard deviation of $0.25 \, ms^{-2}$ was added to the signal.}
        \label{tab:response-noise}
    \end{table}

    Furthermore, the magnitude of the peak was found more accurately by the GP model in this case, with a percentage error of $15.8 \, \%$ versus $38.1 \, \%$ for the FFT, with no change in the angular frequency of the peak for either method.

    A Fourier Transform that is more robust to noise would be more accurate for noisy real world data and therefore would have widespread benefits in mechanical engineering because it would allow for more accurate Frequency Response Functions (FRFs) of structures, and therefore more accurate identification of system properties.
    After the FRF of a structure, such as a wind turbine tower, is determined, it can be evaluated with the same forces that the system is expected to experience to predict the maximum amplitude of vibration expected during its lifetime.
    If this is excessive, the system is at risk of mechanical failure so the design can be changed to prevent this.
    For example, the mass could be changed to move the resonant frequency away from the expected forcing frequency, reducing the expected amplitude of oscillation.
    Alternately, damping could be added to smooth out the resonance peak, reducing its amplitude.
    Therefore, being able to more accurately determine the FRF and the properties of its peaks through this new GP method would be beneficial in the design process because one would be able to more accurately determine how much the design needs to occur (if at all) to reduce the risk of failure to acceptable levels.

    \subsection{Non-uniform Sampling of Frequency Domain}
    Since the Fourier Transform is a continuous function, it can be sampled at any set of points in the domain $(-\infty, \infty)$.
     So, taking the basic dataset and clustering an additional $100$ samples of the spectrum of the GP in the interval of $1 \, rad \, s^{-1}$ containing the peak was able to improve the error in the calculation of the angular frequency of the peak from the same as the FFT ($0.0149 \, rad \, s^{-1}$) down to $0.0039 \, rad \, s^{-1} $.
    However, the error in the magnitude of the peak ($54.2 \, \%$) was still larger than that of the FFT ($22.68 \, \%$)
    If the time domain GP was an exact fit to the latent function, as the interval of angular frequency between samples around the peak tended to zero, so would this error.
    This is not possible with the FFT because the location of the spectrum samples are determined by the time domain samples, both of which must be evenly spaced, meaning that spectrum samples could fall either side of a sharp peak, which creates an uncertainty of $\pm$ half the sample interval.

    Furthermore, since the frequency interval is inversely proportional to the total time period, the difference between the error of the peak of the GP model and the FFT is more pronounced for data over a shorter time period.
    For instance, using only the first 64 points of the same dataset gave the FFT an error in the angular frequency of the peak of $0.574 \, rad \, s^{-1}$ versus $0.054 \, rad \, s^{-1}$ for the GP model, a significant difference.
    In addition, the error in the magnitude of the peak ($60.64 \, \%$) even beat that of the FFT ($685 \, \%$), which performed very poorly for this short dataset.
    This means that this proposed GP model may be better suited for use cases in which there is limited time for testing, such as in the aforementioned example of hypersonic wind tunnel tests of scramjets.

    \subsection{Non-Uniform Sampling of the Time Domain and Overcoming Aliasing and the Nyquist Limit}
    Similarly, the time domain input data need not be uniformly spaced in order to fit the GP\@.
    This would allow the model to be used to find the Fourier Transform of datasets with missing data, which may occur, for example, if an accelerometer placed on a gas turbine engine during a test failed to transmit data for a period of time due to a loose connection.
    If enough data was still present to faithfully represent the system, the GP would not be significantly affected which would allow the test data to still be used, whereas if the standard FFT was used, the test would either need to repeated at added cost, or additional steps would be needed to preprocess the data, adding complication.

    However, more impressively, this feature allows frequencies to be detected that would be above the Nyquist limit for the DFT with the same number of datapoints over the same sampling period.
    This is because for a given time period and number of samples available, many can be bunched in a portion of the time period in order to capture the behaviour of high frequency signal components, leaving a few points over a longer period of time to capture damping information related to the exponential decay of the signal well.
    For example, using an SDOF system with parameters $M = 1 \, kg$; $C = 0.3 \, Nsm^{-1}$; $K = 20,000 \, Nm^{-1}$, the GP model fit to $1024$ datapoints sampled randomly with a triangular distribution over a period of $32$ seconds detected the peak at $141.4 \, rad \, s^{-1}$ with an error of less than $0.1 \, rad \, s^{-1}$.
    The FFT of an equivalent dataset containing the same number of datapoints sampled evenly over the same time period for the same system has a peak at $59.7 \, rad \, s^{-1}$, which is an artifact of aliasing caused by the peak being reflected about the Nyquist limit of $100.5 \, rad \, s^{-1}$.

    This feature of the GP model could be used in contexts where the identification of high frequencies as well as damping information is required, but there are tight constraints on memory such as in the aerospace industry.
    Here, a microsatellite must monitor the heavy vibration it experiences on ascent to orbit to ensure the components onboard haven't exceeded their structural limits, which could indicate damage.
    This would likely include Fourier analysis of accelerometer data to monitor vibration in the frequency domain.
    However, the memory available to do this is limited because storage capacity has weight, every gram of which significantly increases the fuel requirements and cost to get the satellite to orbit.
    Therefore, this ability to detect higher frequencies than an equivalent FFT using the same amount of data and memory would be valuable here.

    \subsection{Computation Time}
    However, one major disadvantage of this GP model is that the computation time scales poorly with large datasets, especially the hyperparameter training phase since it is of $O(N^3)$\nomenclature[L20]{$O()$}{Big O notation, used to describe the complexity - or ``order'' of an algorithm}, which is poor compared to $O(N \log{N})$ for the Fast Fourier Transform (FFT).
    This means that the model handles poorly datasets consisting of more than a few thousand points, and even at this scale, fitting the GP to the time domain then converting this to the frequency domain takes several orders of magnitude longer ($0.84 \, s$) than the time the FFT ($7.5 \times 10^{-5}s$) takes run.
    This performance is poor considering that it is not uncommon to need to work with datasets consisting of up to a million datapoints in industry, such as during a tap test for modal analysis of a turbine blade, for which the GP model in its current state would be unsuitable due to the excessive time and cost computational resources that would be required.

    \section{Directions for Future Work}
    \subsection{Fourier Transform of the Covariance}
    Unlike the DFT, the GP provides some uncertainty data the standard deviation of the prediction of the spectrum can be found using the GP\@.
    This is derived from the main diagonal of the covariance matrix and is always positive and real (see Mercer's theorem~\cite{rasmussen2006gaussian}, p.96), meaning that its phase is always zero.
    Note that since each amplitude prediction is a Gaussian distribution, which extends infinitely in both directions, some portion of the distribution will always fall below zero.
    This is unusual because amplitudes are usually positive only, but this portion of the distribution can be interpreted as simply a positive amplitude of the same magnitude but the opposite phase.

    This standard deviation prediction allows one to convert to the frequency domain information about the uncertainty of the magnitude of vibrations, provided that the GP in the time domain fits the noise in the training data.
    In the automotive industry, a piston connecting (``con'') rod in an internal combustion engine undergoes heavy vibration of very many cycles (twice for each revolution of a four-stroke engine), so fatigue analysis is crucial to ensure that it does not fail from crack formation and propagation before the end of the design life of the vehicle.
    Since they are typically made of a ferrous alloy such as forged steel, they have a fatigue limit~\cite{RoymechFatigue}, which is a distinct stress level for a given frequency below which failure from fatigue will not occur - even from an infinite number of loading cycles~\cite{BeerJohnston1992}.
    This stress level can be calculated from experimental acceleration data, and therefore it can be shown that the stress on the con rod is below the fatigue limit by showing that the magnitude of acceleration across the range of frequencies is below its corresponding limit.
    Testing this would be valuable because one could show that the con rod will not fail form fatigue if the test passes.

    However, if the FFT was used instead, even if the nominal calculated value was below its limit, the uncertainty interval would be unknown and so it would be unclear whether or not the upper bound of the range of possible values is above the fatigue limit.
    Therefore, one could not confidently conclude that the acceleration will be below the fatigue limit without a conservative factor of safety, which could add unnecessary weight used to make the con rod stronger than it needs to be.
    In contrast, since the GP provides the acceleration at a given frequency as a Gaussian distribution, the uncertainty information is encoded in standard deviation of the distribution.
    Therefore, it would be possible to calculate the probability that the acceleration falls over the limit and if this chance is sufficiently low, one can conclude that the component is below the fatigue limit without the need for a conservative safety factor, increasing the performance of the engine.

    \textcolor{red}{For this dataset, the covariance function was nearly zero amplitude at all frequencies except for a narrow spike around zero frequency between -xxx and xxx rad/s, reaching a maximum of xxx at - rad/s.
    This spike around zero makes intuitive sense because the amplitude of very low frequencies, the number of cycles they will experience in a given time period will tend to zero so there will be limited data to deduce the magnitude.
    However,...
    The fact that the standard deviation frequency domain amplitude predictions are near zero for all other frequencies suggests that the Fourier Transform is defined precisely, although the accuracy depends on how well the GP fits the time domain data.}

    \subsection{Scaling to large data}
    Although this new Gaussian Process (GP) method of performing the Fourier Transform is promising, the fact that it scales to large datasets so poorly means that it is impractical in its current state.
    To address this, a sparse GP approximation could be employed to allow both the NLML and predict functions to scale to large data

    One type of sparse GP approximation is the Fully Independent Training Conditional (FITC)~\cite{q-candela}.
    Successful implementation of this can be confirmed if the NLML is the same for the basic GP and its sparse approximation when the set of inducing points is exactly the same as the test dataset.
    This would reduce the complexity of the training phase from $O(N^3)$ to $O(MN^2)$, where M is a set of inducing points chosen to be a subset of the original training points, evenly spaced, and $N \gg M$. \nomenclature[L17]{$M$}{Number of inducing points in Sparse GP}
    After this is complete, the hyperparameters could be optimised and the GP can be fit for functions containing more than $10,000$ data points in a significantly reduced computation time.

     This was originally an objective to complete before extending the model to the frequency domain in Objective~\ref{item:FT}.
    It was attempted extensively, but an implementation that was numerically stable enough to produce NLML values consistent with the basic GP could not be complete, and thus it failed to converge to fit the test data.
    This was likely due to the matrix operations involved such as determinants and inverses that can be unstable for large matrices.

    To progress this method further, one may have better performance implementing a different sparse GP approximation such as the variational free energy (VFE) approach instead, which works by approximating the posterior distribution, rather than the prior~\cite{murphy2023probabilistic}.

%    \subsection{and another subsection}
%    \textcolor{red}{ New kernel types; multi degree of freedom / dimension GP's!!!!; different solver types? etc.}

    \section{Conclusions}
    Overall, This paper proposes the use of a Gaussian Process (GP) model to perform the Fourier Transform.
    It shows this is a valid alternative to the industry standard Fast Fourier Transform (FFT) which exhibited significantly better robustness to noise by a factor of 6.8, as well as several promising features that made it more versatile in the context of dynamic systems in mechanical engineering.
    Most notably, the fact that the time domain input data need not be evenly spaced enables the model to detect frequencies that would be above the Nyquist limit of an equivalent FFT of the same number of datapoints sampled evenly across the same time period.
    This gives the model potential to exceed the performance of the FFT in industrial contexts where data is noisy, and there are constraints to the amount of data that can be collected due to cost or availability, or limited memory.
    However, the computational speed of this GP model is currently orders of magnitude slower than the FFT, so before it can be a viable alternative in practice, it would need development to allow it to scale better to large datasets.

    \FloatBarrier

    \newpage
    \printbibliography
    \addcontentsline{toc}{section}{References}
    \newpage
    \input{appendix}

\end{document}